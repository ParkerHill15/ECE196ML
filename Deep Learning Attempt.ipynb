{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to format data into data points\n",
    "def makeDataPoints(data):\n",
    "    newDataPoints=np.empty([len(data)-1,12])\n",
    "    newTargets=np.empty([len(data)-1,1])\n",
    "    for g in range(len(data)):\n",
    "        datapoint = 0\n",
    "        d0,fp0 =data[g,1], data[g,2]\n",
    "        if g == 1:\n",
    "            d1,fp1=data[g-1,1], data[g-1,2]\n",
    "            datapoint=np.array([g,d0,d1,fp1,d1,fp1,d1,fp1,d1,fp1,d1,fp1])\n",
    "        if g == 2:\n",
    "            d1,fp1=data[g-1,1], data[g-1,2]\n",
    "            d2,fp2=data[g-2,1], data[g-2,2]\n",
    "            datapoint=np.array([g,d0,d1,fp1,d1,fp1,d1,fp1,d1,fp1,d2,fp2])\n",
    "        if g == 3:\n",
    "            d1,fp1=data[g-1,1], data[g-1,2]\n",
    "            d2,fp2=data[g-2,1], data[g-2,2]\n",
    "            d3,fp3=data[g-3,1], data[g-3,2]\n",
    "            datapoint=np.array([g,d0,d1,fp1,d1,fp1,d1,fp1,d2,fp2,d3,fp3])\n",
    "        if g == 4:\n",
    "            d1,fp1=data[g-1,1], data[g-1,2]\n",
    "            d2,fp2=data[g-2,1], data[g-2,2]\n",
    "            d3,fp3=data[g-3,1], data[g-3,2]\n",
    "            d4,fp4=data[g-4,1], data[g-4,2]\n",
    "            datapoint=np.array([g,d0,d1,fp1,d1,fp1,d2,fp2,d3,fp3,d4,fp4])\n",
    "        if g >= 5:\n",
    "            d1,fp1=data[g-1,1], data[g-1,2]\n",
    "            d2,fp2=data[g-2,1], data[g-2,2]\n",
    "            d3,fp3=data[g-3,1], data[g-3,2]\n",
    "            d4,fp4=data[g-4,1], data[g-4,2]\n",
    "            d5,fp5=data[g-5,1], data[g-5,2]\n",
    "            datapoint=np.array([g,d0,d1,fp1,d2,fp2,d3,fp3,d4,fp4,d5,fp5])\n",
    "        newTargets[g-1]=fp0\n",
    "        newDataPoints[g-1]=datapoint\n",
    "    return newDataPoints, newTargets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.   7.  31.  ... 23.1 31.  23.1]\n",
      " [ 2.  14.   7.  ... 25.4 31.  23.1]\n",
      " [ 3.  28.  14.  ... 25.4 31.  23.1]\n",
      " ...\n",
      " [ 8.   3.  11.  ...  5.  15.  10.9]\n",
      " [ 9.   6.   3.  ...  7.9 20.   5. ]\n",
      " [10.   1.   6.  ... 11.4 32.   7.9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5224, 12)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making data into useable numpy arrays to be passed into the final dataset creation filter\n",
    "data = pd.read_csv(r'C:\\Users\\Parker Hill\\Desktop\\ECE 196\\NFL Running Back\\FinalData.csv') #Change to directory that your data is in\n",
    "data = pd.DataFrame(data)\n",
    "length = len(data['Week'])\n",
    "previousweeknum = -1\n",
    "array = np.empty([1,3])\n",
    "dataPoints = np.empty([1,12])\n",
    "targets = np.empty([1,1])\n",
    "for i in range(0,length):\n",
    "    weeknum = data['Week'][i]\n",
    "    if weeknum < previousweeknum:\n",
    "        array = np.delete(array,(0),axis=0) #I dunno how to make a truly empty array, so need to delete the first row each time\n",
    "        #This line you would call your make data points function on with the created array, I used a print statement for verification\n",
    "        newDataPoints,newTargets = makeDataPoints(array)\n",
    "        dataPoints = np.vstack([dataPoints,newDataPoints])\n",
    "        targets = np.vstack([targets,newTargets])\n",
    "        #print(array)\n",
    "        array = np.empty([1,3])\n",
    "    rank = data['Rank'][i]\n",
    "    fantasy = data['Fantasy'][i]\n",
    "    newrow = np.array([weeknum, rank, fantasy])\n",
    "    array = np.vstack([array, newrow])\n",
    "    previousweeknum = weeknum\n",
    "dataPoints = np.delete(dataPoints,(0),axis=0)\n",
    "targets = np.delete(targets,(0),axis=0)\n",
    "print(dataPoints)\n",
    "dataPoints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.   7.  31.  ... 23.1 31.  23.1]\n",
      " [ 2.  14.   7.  ... 25.4 31.  23.1]\n",
      " [ 3.  28.  14.  ... 25.4 31.  23.1]\n",
      " ...\n",
      " [ 4.   8.  31.  ...  4.1 15.   3.1]\n",
      " [ 5.  10.   8.  ...  4.1 15.   3.1]\n",
      " [ 6.   6.  10.  ... 21.4  9.   4.1]]\n",
      "[[25.4]\n",
      " [23.4]\n",
      " [16.6]\n",
      " ...\n",
      " [15.8]\n",
      " [ 9.4]\n",
      " [ 9.2]]\n"
     ]
    }
   ],
   "source": [
    "traindata_x = dataPoints[0:4000]\n",
    "traindata_y = targets[0:4000]\n",
    "testdata_x = dataPoints[4001:5224]\n",
    "testdata_y = targets[4001:5224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://www.pluralsight.com/guides/predictive-analytics-with-pytorch\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        input_dim = 12\n",
    "        output_dim = 1\n",
    "        super(ANN,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,64)\n",
    "        self.fc2 = nn.Linear(64,64)\n",
    "        self.fc3 = nn.Linear(64,32)\n",
    "        self.fc4 = nn.Linear(32,32)\n",
    "        self.output_layer = nn.Linear(32,output_dim)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.output_layer(x)\n",
    "        return nn.Sigmoid()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc1): Linear(in_features=12, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (output_layer): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.15, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ANN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4000, 1])\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.from_numpy(traindata_x)\n",
    "Y_train = torch.from_numpy(traindata_y)\n",
    "X_test = torch.from_numpy(testdata_x)\n",
    "Y_test = torch.from_numpy(testdata_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x00000186463694E0>\n"
     ]
    }
   ],
   "source": [
    "train = torch.utils.data.TensorDataset(X_train,Y_train)\n",
    "test = torch.utils.data.TensorDataset(X_test,Y_test)\n",
    "train_loader = torch.utils.data.DataLoader(train,batch_size = 64, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test,batch_size = 64, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, weight_decay = 1e-6, momentum = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 2 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 3 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 4 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 5 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 6 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 7 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 8 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 9 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 10 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 11 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 12 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 13 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 14 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 15 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 16 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 17 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 18 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 19 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 20 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 21 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 22 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 23 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 24 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 25 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 26 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 27 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 28 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 29 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 30 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 31 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 32 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 33 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 34 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 35 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 36 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 37 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 38 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 39 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 40 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 41 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 42 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 43 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 44 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 45 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 46 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 47 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 48 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 49 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 50 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 51 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 52 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 53 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 54 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 55 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 56 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 57 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 58 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 59 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 60 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 61 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 62 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 63 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 64 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 65 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 66 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 67 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 68 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 69 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 70 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 71 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 72 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 73 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 74 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 75 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 76 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 77 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 78 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 79 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 80 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 81 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 82 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 83 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 84 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 85 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 86 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 87 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 88 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 89 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 90 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 91 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 92 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 93 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 94 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 95 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 96 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 97 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 98 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 99 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 100 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 101 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 102 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 103 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 104 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 105 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 106 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 107 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 108 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 109 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 110 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 111 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 112 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 113 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 114 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 115 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 116 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 117 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 118 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 119 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 120 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 121 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 122 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 123 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 124 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 125 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 126 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 127 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 128 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 129 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 130 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 131 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 132 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 133 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 134 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 135 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 136 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 137 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 138 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 139 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 140 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 141 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 142 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 143 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 144 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 145 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 146 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 147 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 148 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 149 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 150 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 151 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 152 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 153 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 154 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 155 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 156 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 157 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 158 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 159 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 160 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 161 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 162 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 163 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 164 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 165 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 166 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 167 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 168 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 169 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 170 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 171 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 172 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 173 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 174 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 175 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 176 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 177 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 178 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 179 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 180 \tTraining Loss: 0.0000\t Acc: 0.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 181 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 182 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 183 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 184 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 185 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 186 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 187 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 188 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 189 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 190 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 191 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 192 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 193 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 194 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 195 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 196 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 197 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 198 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 199 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 200 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 201 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 202 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 203 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 204 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 205 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 206 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 207 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 208 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 209 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 210 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 211 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 212 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 213 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 214 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 215 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 216 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 217 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 218 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 219 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 220 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 221 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 222 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 223 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 224 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 225 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 226 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 227 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 228 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 229 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 230 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 231 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 232 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 233 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 234 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 235 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 236 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 237 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 238 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 239 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 240 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 241 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 242 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 243 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 244 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 245 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 246 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 247 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 248 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 249 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 250 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 251 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 252 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 253 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 254 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 255 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 256 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 257 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 258 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 259 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 260 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 261 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 262 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 263 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 264 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 265 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 266 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 267 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 268 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 269 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 270 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 271 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 272 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 273 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 274 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 275 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 276 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 277 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 278 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 279 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 280 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 281 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 282 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 283 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 284 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 285 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 286 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 287 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 288 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 289 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 290 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 291 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 292 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 293 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 294 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 295 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 296 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 297 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 298 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 299 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 300 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 301 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 302 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 303 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 304 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 305 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 306 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 307 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 308 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 309 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 310 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 311 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 312 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 313 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 314 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 315 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 316 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 317 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 318 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 319 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 320 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 321 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 322 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 323 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 324 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 325 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 326 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 327 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 328 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 329 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 330 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 331 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 332 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 333 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 334 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 335 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 336 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 337 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 338 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 339 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 340 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 341 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 342 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 343 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 344 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 345 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 346 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 347 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 348 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 349 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 350 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 351 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 352 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 353 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 354 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 355 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 356 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 357 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 358 \tTraining Loss: 0.0000\t Acc: 0.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 359 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 360 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 361 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 362 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 363 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 364 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 365 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 366 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 367 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 368 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 369 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 370 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 371 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 372 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 373 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 374 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 375 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 376 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 377 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 378 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 379 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 380 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 381 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 382 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 383 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 384 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 385 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 386 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 387 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 388 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 389 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 390 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 391 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 392 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 393 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 394 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 395 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 396 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 397 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 398 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 399 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 400 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 401 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 402 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 403 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 404 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 405 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 406 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 407 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 408 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 409 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 410 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 411 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 412 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 413 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 414 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 415 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 416 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 417 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 418 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 419 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 420 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 421 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 422 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 423 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 424 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 425 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 426 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 427 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 428 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 429 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 430 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 431 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 432 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 433 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 434 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 435 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 436 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 437 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 438 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 439 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 440 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 441 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 442 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 443 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 444 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 445 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 446 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 447 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 448 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 449 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 450 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 451 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 452 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 453 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 454 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 455 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 456 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 457 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 458 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 459 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 460 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 461 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 462 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 463 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 464 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 465 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 466 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 467 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 468 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 469 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 470 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 471 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 472 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 473 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 474 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 475 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 476 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 477 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 478 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 479 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 480 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 481 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 482 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 483 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 484 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 485 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 486 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 487 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 488 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 489 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 490 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 491 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 492 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 493 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 494 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 495 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 496 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 497 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 498 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 499 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 500 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 501 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 502 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 503 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 504 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 505 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 506 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 507 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 508 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 509 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 510 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 511 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 512 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 513 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 514 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 515 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 516 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 517 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 518 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 519 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 520 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 521 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 522 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 523 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 524 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 525 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 526 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 527 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 528 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 529 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 530 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 531 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 532 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 533 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 534 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 535 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 536 \tTraining Loss: 0.0000\t Acc: 0.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 537 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 538 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 539 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 540 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 541 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 542 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 543 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 544 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 545 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 546 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 547 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 548 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 549 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 550 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 551 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 552 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 553 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 554 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 555 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 556 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 557 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 558 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 559 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 560 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 561 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 562 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 563 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 564 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 565 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 566 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 567 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 568 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 569 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 570 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 571 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 572 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 573 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 574 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 575 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 576 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 577 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 578 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 579 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 580 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 581 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 582 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 583 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 584 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 585 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 586 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 587 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 588 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 589 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 590 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 591 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 592 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 593 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 594 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 595 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 596 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 597 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 598 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 599 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 600 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 601 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 602 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 603 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 604 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 605 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 606 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 607 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 608 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 609 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 610 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 611 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 612 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 613 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 614 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 615 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 616 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 617 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 618 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 619 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 620 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 621 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 622 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 623 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 624 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 625 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 626 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 627 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 628 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 629 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 630 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 631 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 632 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 633 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 634 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 635 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 636 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 637 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 638 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 639 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 640 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 641 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 642 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 643 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 644 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 645 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 646 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 647 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 648 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 649 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 650 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 651 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 652 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 653 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 654 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 655 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 656 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 657 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 658 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 659 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 660 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 661 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 662 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 663 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 664 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 665 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 666 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 667 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 668 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 669 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 670 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 671 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 672 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 673 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 674 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 675 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 676 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 677 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 678 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 679 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 680 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 681 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 682 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 683 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 684 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 685 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 686 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 687 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 688 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 689 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 690 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 691 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 692 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 693 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 694 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 695 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 696 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 697 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 698 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 699 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 700 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 701 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 702 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 703 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 704 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 705 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 706 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 707 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 708 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 709 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 710 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 711 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 712 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 713 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 714 \tTraining Loss: 0.0000\t Acc: 0.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 715 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 716 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 717 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 718 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 719 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 720 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 721 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 722 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 723 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 724 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 725 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 726 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 727 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 728 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 729 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 730 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 731 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 732 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 733 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 734 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 735 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 736 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 737 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 738 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 739 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 740 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 741 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 742 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 743 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 744 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 745 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 746 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 747 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 748 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 749 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 750 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 751 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 752 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 753 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 754 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 755 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 756 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 757 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 758 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 759 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 760 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 761 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 762 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 763 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 764 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 765 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 766 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 767 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 768 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 769 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 770 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 771 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 772 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 773 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 774 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 775 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 776 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 777 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 778 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 779 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 780 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 781 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 782 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 783 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 784 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 785 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 786 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 787 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 788 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 789 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 790 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 791 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 792 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 793 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 794 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 795 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 796 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 797 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 798 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 799 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 800 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 801 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 802 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 803 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 804 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 805 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 806 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 807 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 808 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 809 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 810 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 811 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 812 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 813 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 814 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 815 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 816 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 817 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 818 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 819 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 820 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 821 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 822 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 823 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 824 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 825 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 826 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 827 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 828 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 829 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 830 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 831 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 832 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 833 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 834 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 835 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 836 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 837 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 838 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 839 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 840 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 841 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 842 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 843 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 844 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 845 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 846 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 847 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 848 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 849 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 850 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 851 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 852 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 853 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 854 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 855 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 856 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 857 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 858 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 859 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 860 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 861 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 862 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 863 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 864 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 865 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 866 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 867 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 868 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 869 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 870 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 871 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 872 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 873 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 874 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 875 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 876 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 877 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 878 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 879 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 880 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 881 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 882 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 883 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 884 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 885 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 886 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 887 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 888 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 889 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 890 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 891 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 892 \tTraining Loss: 0.0000\t Acc: 0.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 893 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 894 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 895 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 896 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 897 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 898 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 899 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 900 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 901 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 902 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 903 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 904 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 905 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 906 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 907 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 908 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 909 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 910 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 911 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 912 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 913 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 914 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 915 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 916 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 917 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 918 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 919 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 920 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 921 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 922 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 923 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 924 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 925 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 926 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 927 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 928 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 929 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 930 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 931 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 932 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 933 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 934 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 935 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 936 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 937 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 938 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 939 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 940 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 941 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 942 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 943 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 944 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 945 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 946 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 947 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 948 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 949 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 950 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 951 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 952 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 953 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 954 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 955 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 956 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 957 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 958 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 959 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 960 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 961 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 962 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 963 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 964 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 965 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 966 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 967 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 968 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 969 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 970 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 971 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 972 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 973 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 974 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 975 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 976 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 977 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 978 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 979 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 980 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 981 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 982 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 983 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 984 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 985 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 986 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 987 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 988 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 989 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 990 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 991 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 992 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 993 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 994 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 995 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 996 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 997 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 998 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 999 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1000 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1001 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1002 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1003 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1004 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1005 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1006 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1007 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1008 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1009 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1010 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1011 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1012 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1013 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1014 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1015 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1016 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1017 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1018 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1019 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1020 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1021 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1022 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1023 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1024 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1025 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1026 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1027 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1028 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1029 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1030 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1031 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1032 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1033 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1034 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1035 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1036 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1037 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1038 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1039 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1040 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1041 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1042 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1043 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1044 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1045 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1046 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1047 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1048 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1049 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1050 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1051 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1052 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1053 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1054 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1055 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1056 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1057 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1058 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1059 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1060 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1061 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1062 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1063 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1064 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1065 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1066 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1067 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1068 \tTraining Loss: 0.0000\t Acc: 0.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1069 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1070 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1071 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1072 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1073 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1074 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1075 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1076 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1077 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1078 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1079 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1080 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1081 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1082 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1083 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1084 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1085 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1086 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1087 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1088 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1089 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1090 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1091 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1092 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1093 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1094 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1095 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1096 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1097 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1098 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1099 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1100 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1101 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1102 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1103 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1104 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1105 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1106 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1107 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1108 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1109 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1110 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1111 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1112 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1113 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1114 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1115 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1116 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1117 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1118 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1119 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1120 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1121 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1122 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1123 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1124 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1125 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1126 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1127 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1128 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1129 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1130 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1131 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1132 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1133 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1134 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1135 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1136 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1137 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1138 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1139 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1140 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1141 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1142 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1143 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1144 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1145 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1146 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1147 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1148 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1149 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1150 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1151 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1152 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1153 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1154 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1155 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1156 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1157 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1158 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1159 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1160 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1161 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1162 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1163 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1164 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1165 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1166 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1167 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1168 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1169 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1170 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1171 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1172 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1173 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1174 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1175 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1176 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1177 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1178 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1179 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1180 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1181 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1182 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1183 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1184 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1185 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1186 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1187 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1188 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1189 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1190 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1191 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1192 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1193 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1194 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1195 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1196 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1197 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1198 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1199 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1200 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1201 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1202 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1203 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1204 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1205 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1206 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1207 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1208 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1209 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1210 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1211 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1212 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1213 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1214 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1215 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1216 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1217 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1218 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1219 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1220 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1221 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1222 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1223 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1224 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1225 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1226 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1227 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1228 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1229 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1230 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1231 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1232 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1233 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1234 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1235 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1236 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1237 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1238 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1239 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1240 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1241 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1242 \tTraining Loss: 0.0000\t Acc: 0.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1243 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1244 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1245 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1246 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1247 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1248 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1249 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1250 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1251 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1252 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1253 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1254 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1255 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1256 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1257 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1258 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1259 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1260 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1261 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1262 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1263 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1264 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1265 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1266 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1267 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1268 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1269 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1270 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1271 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1272 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1273 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1274 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1275 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1276 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1277 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1278 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1279 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1280 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1281 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1282 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1283 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1284 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1285 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1286 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1287 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1288 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1289 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1290 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1291 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1292 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1293 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1294 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1295 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1296 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1297 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1298 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1299 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1300 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1301 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1302 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1303 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1304 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1305 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1306 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1307 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1308 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1309 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1310 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1311 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1312 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1313 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1314 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1315 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1316 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1317 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1318 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1319 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1320 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1321 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1322 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1323 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1324 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1325 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1326 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1327 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1328 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1329 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1330 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1331 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1332 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1333 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1334 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1335 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1336 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1337 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1338 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1339 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1340 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1341 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1342 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1343 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1344 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1345 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1346 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1347 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1348 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1349 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1350 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1351 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1352 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1353 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1354 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1355 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1356 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1357 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1358 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1359 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1360 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1361 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1362 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1363 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1364 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1365 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1366 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1367 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1368 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1369 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1370 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1371 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1372 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1373 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1374 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1375 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1376 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1377 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1378 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1379 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1380 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1381 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1382 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1383 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1384 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1385 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1386 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1387 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1388 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1389 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1390 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1391 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1392 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1393 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1394 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1395 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1396 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1397 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1398 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1399 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1400 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1401 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1402 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1403 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1404 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1405 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1406 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1407 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1408 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1409 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1410 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1411 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1412 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1413 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1414 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1415 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1416 \tTraining Loss: 0.0000\t Acc: 0.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1417 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1418 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1419 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1420 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1421 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1422 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1423 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1424 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1425 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1426 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1427 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1428 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1429 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1430 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1431 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1432 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1433 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1434 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1435 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1436 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1437 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1438 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1439 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1440 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1441 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1442 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1443 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1444 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1445 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1446 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1447 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1448 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1449 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1450 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1451 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1452 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1453 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1454 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1455 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1456 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1457 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1458 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1459 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1460 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1461 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1462 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1463 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1464 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1465 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1466 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1467 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1468 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1469 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1470 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1471 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1472 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1473 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1474 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1475 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1476 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1477 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1478 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1479 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1480 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1481 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1482 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1483 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1484 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1485 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1486 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1487 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1488 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1489 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1490 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1491 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1492 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1493 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1494 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1495 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1496 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1497 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1498 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1499 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1500 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1501 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1502 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1503 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1504 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1505 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1506 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1507 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1508 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1509 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1510 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1511 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1512 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1513 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1514 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1515 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1516 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1517 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1518 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1519 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1520 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1521 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1522 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1523 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1524 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1525 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1526 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1527 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1528 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1529 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1530 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1531 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1532 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1533 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1534 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1535 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1536 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1537 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1538 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1539 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1540 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1541 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1542 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1543 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1544 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1545 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1546 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1547 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1548 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1549 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1550 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1551 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1552 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1553 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1554 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1555 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1556 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1557 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1558 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1559 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1560 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1561 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1562 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1563 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1564 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1565 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1566 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1567 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1568 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1569 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1570 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1571 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1572 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1573 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1574 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1575 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1576 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1577 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1578 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1579 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1580 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1581 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1582 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1583 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1584 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1585 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1586 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1587 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1588 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1589 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1590 \tTraining Loss: 0.0000\t Acc: 0.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1591 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1592 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1593 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1594 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1595 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1596 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1597 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1598 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1599 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1600 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1601 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1602 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1603 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1604 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1605 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1606 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1607 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1608 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1609 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1610 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1611 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1612 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1613 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1614 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1615 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1616 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1617 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1618 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1619 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1620 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1621 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1622 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1623 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1624 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1625 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1626 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1627 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1628 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1629 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1630 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1631 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1632 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1633 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1634 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1635 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1636 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1637 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1638 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1639 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1640 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1641 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1642 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1643 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1644 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1645 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1646 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1647 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1648 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1649 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1650 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1651 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1652 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1653 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1654 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1655 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1656 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1657 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1658 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1659 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1660 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1661 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1662 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1663 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1664 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1665 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1666 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1667 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1668 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1669 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1670 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1671 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1672 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1673 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1674 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1675 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1676 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1677 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1678 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1679 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1680 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1681 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1682 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1683 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1684 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1685 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1686 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1687 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1688 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1689 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1690 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1691 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1692 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1693 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1694 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1695 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1696 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1697 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1698 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1699 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1700 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1701 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1702 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1703 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1704 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1705 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1706 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1707 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1708 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1709 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1710 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1711 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1712 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1713 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1714 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1715 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1716 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1717 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1718 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1719 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1720 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1721 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1722 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1723 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1724 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1725 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1726 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1727 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1728 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1729 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1730 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1731 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1732 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1733 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1734 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1735 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1736 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1737 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1738 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1739 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1740 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1741 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1742 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1743 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1744 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1745 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1746 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1747 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1748 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1749 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1750 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1751 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1752 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1753 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1754 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1755 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1756 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1757 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1758 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1759 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1760 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1761 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1762 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1763 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1764 \tTraining Loss: 0.0000\t Acc: 0.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1765 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1766 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1767 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1768 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1769 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1770 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1771 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1772 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1773 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1774 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1775 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1776 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1777 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1778 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1779 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1780 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1781 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1782 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1783 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1784 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1785 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1786 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1787 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1788 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1789 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1790 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1791 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1792 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1793 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1794 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1795 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1796 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1797 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1798 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1799 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1800 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1801 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1802 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1803 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1804 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1805 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1806 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1807 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1808 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1809 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1810 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1811 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1812 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1813 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1814 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1815 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1816 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1817 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1818 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1819 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1820 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1821 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1822 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1823 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1824 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1825 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1826 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1827 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1828 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1829 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1830 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1831 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1832 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1833 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1834 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1835 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1836 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1837 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1838 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1839 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1840 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1841 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1842 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1843 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1844 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1845 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1846 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1847 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1848 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1849 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1850 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1851 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1852 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1853 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1854 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1855 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1856 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1857 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1858 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1859 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1860 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1861 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1862 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1863 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1864 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1865 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1866 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1867 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1868 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1869 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1870 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1871 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1872 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1873 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1874 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1875 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1876 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1877 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1878 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1879 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1880 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1881 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1882 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1883 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1884 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1885 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1886 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1887 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1888 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1889 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1890 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1891 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1892 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1893 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1894 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1895 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1896 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1897 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1898 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1899 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1900 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1901 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1902 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1903 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1904 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1905 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1906 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1907 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1908 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1909 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1910 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1911 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1912 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1913 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1914 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1915 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1916 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1917 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1918 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1919 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1920 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1921 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1922 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1923 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1924 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1925 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1926 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1927 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1928 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1929 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1930 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1931 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1932 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1933 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1934 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1935 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1936 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1937 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1938 \tTraining Loss: 0.0000\t Acc: 0.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1939 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1940 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1941 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1942 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1943 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1944 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1945 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1946 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1947 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1948 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1949 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1950 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1951 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1952 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1953 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1954 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1955 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1956 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1957 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1958 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1959 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1960 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1961 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1962 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1963 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1964 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1965 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1966 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1967 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1968 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1969 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1970 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1971 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1972 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1973 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1974 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1975 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1976 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1977 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1978 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1979 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1980 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1981 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1982 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1983 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1984 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1985 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1986 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1987 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1988 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1989 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1990 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1991 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1992 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1993 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1994 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1995 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1996 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1997 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1998 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 1999 \tTraining Loss: 0.0000\t Acc: 0.22%\n",
      "Epoch: 2000 \tTraining Loss: 0.0000\t Acc: 0.22%\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "epoch_list = []\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "# lines 7 onwards\n",
    "model.train() # prepare model for training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    valloss = 0.0\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data,target in train_loader:\n",
    "        data = Variable(data).float()\n",
    "        target = Variable(target).type(torch.FloatTensor)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        predicted = (torch.round(output.data[0]))\n",
    "        total += len(target)\n",
    "        correct += (predicted == target).sum()\n",
    "\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainloss += loss.item()*data.size(0)\n",
    "\n",
    "    trainloss = trainloss/len(train_loader.dataset)\n",
    "    accuracy = 100 * correct / float(total)\n",
    "    train_acc_list.append(accuracy)\n",
    "    train_loss_list.append(train_loss)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f}\\t Acc: {:.2f}%'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        accuracy\n",
    "        ))\n",
    "    epoch_list.append(epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4088)\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "valloss = 0\n",
    "model.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data = Variable(data).float()\n",
    "        target = Variable(target).type(torch.FloatTensor)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        valloss += loss.item()*data.size(0)\n",
    "        \n",
    "        predicted = (torch.round(output.data[0]))\n",
    "        total += len(target)\n",
    "        correct += (predicted == target).sum()\n",
    "    \n",
    "    valloss = valloss/len(test_loader.dataset)\n",
    "    accuracy = 100 * correct/ float(total)\n",
    "    print(accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
